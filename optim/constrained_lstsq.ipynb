{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-wealth",
   "metadata": {},
   "outputs": [],
   "source": [
    "from proximal_operators_and_gradients import (\n",
    "    gen_matrices, proj_linf_annulus, proj_simplex, proj_l1_ball)\n",
    "from numpy.linalg import norm, inv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-neutral",
   "metadata": {},
   "source": [
    "# Grad simplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-stake",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj_grad_dirty(mat, vec, err_max, ite_max):\n",
    "    dim = len(mat)\n",
    "    sqr = np.matmul(mat.T, mat)\n",
    "    eta = 1 / norm(sqr)  # mat is not zero\n",
    "    mul_mat = np.eye(dim) - eta * sqr\n",
    "    out = eta * np.matmul(mat.T, vec)\n",
    "    prim = np.zeros(dim)\n",
    "    err = err_max + 1\n",
    "    ite = 0\n",
    "    errs = list()\n",
    "    while err > err_max and ite < ite_max:\n",
    "        prim_new = proj_simplex(np.matmul(mul_mat, prim) + out, 1)\n",
    "        err = norm(prim - prim_new)\n",
    "        errs.append([err, norm(mat @ prim_new - vec)])\n",
    "        prim = prim_new.copy()\n",
    "        ite += 1\n",
    "    return prim, errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-quilt",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat, vec = gen_matrices(1000)[0:2]\n",
    "res, errs = proj_grad_dirty(mat, vec, 1e-5, 10000)\n",
    "errs = np.array(errs)\n",
    "errs[:, 1] = errs[:, 1] - errs[-1, 1]\n",
    "plt.plot(np.log(errs))\n",
    "res.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-screw",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj_grad_simplex(mat, vec, rad=1, err_max=1e-5, ite_max=1000):\n",
    "    dim = len(mat)\n",
    "    sqr = np.matmul(mat.T, mat)\n",
    "    eta = 1 / norm(sqr)  # mat is not zero\n",
    "    mul_mat = np.eye(dim) - eta * sqr\n",
    "    out = eta * np.matmul(mat.T, vec)\n",
    "    prim = np.zeros(dim)\n",
    "    err = err_max + 1\n",
    "    ite = 0\n",
    "    errs = list()\n",
    "    while err > err_max and ite < ite_max:\n",
    "        prim_new = proj_simplex(np.matmul(mul_mat, prim) + out, rad)\n",
    "        err = norm(prim - prim_new)\n",
    "        errs.append(err)\n",
    "        prim = prim_new.copy()\n",
    "        ite += 1\n",
    "    return prim, errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-onion",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "mat, vec = gen_matrices(1000)[0:2]\n",
    "res, errs = proj_grad_simplex(mat, vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-execution",
   "metadata": {},
   "source": [
    "# Grad admm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-oxygen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_admm_dirty(mat, vec, rad_1, rad_infty, err_max, ite_max):\n",
    "    dim = len(mat)\n",
    "    sqr = np.matmul(mat.T, mat)\n",
    "    eta = 1 / norm(sqr)  # mat is not zero\n",
    "    mul_mat = np.eye(dim) - eta * sqr\n",
    "    out = eta * np.matmul(mat.T, vec)\n",
    "    concensus = np.zeros(dim)\n",
    "    dual_1 = np.zeros(dim)\n",
    "    dual_2 = np.zeros(dim)\n",
    "#     v_min = -rad_infty\n",
    "#     v_max = +rad_infty\n",
    "    err = err_max + 1\n",
    "    ite = 0\n",
    "    errs = list()\n",
    "    while err > err_max and ite < ite_max:\n",
    "        prim = np.matmul(mul_mat, concensus) + out\n",
    "        prim_1 = proj_linf_annulus(\n",
    "            concensus - dual_1, -rad_infty, +rad_infty)\n",
    "        prim_2 = proj_l1_ball(concensus - dual_2, rad_1)\n",
    "        concensus_new = 1 / 3 * (prim + prim_1 + prim_2)\n",
    "        dual_1_new = dual_1 + prim_1 - concensus\n",
    "        dual_2_new = dual_2 + prim_2 - concensus\n",
    "        err_prim = [\n",
    "            norm(concensus_new - prim),\n",
    "            norm(concensus_new - prim_1),\n",
    "            norm(concensus_new - prim_2),\n",
    "        ]\n",
    "        err_dual = [\n",
    "            norm(concensus - concensus_new),\n",
    "            norm(prim_1 - concensus),\n",
    "            norm(prim_2 - concensus)]\n",
    "        err = max(err_prim + err_dual)\n",
    "        errs.append(err_prim + err_dual + [norm(mat @ concensus - vec)])\n",
    "        concensus = concensus_new.copy()\n",
    "        dual_1 = dual_1_new.copy()\n",
    "        dual_2 = dual_2_new.copy()\n",
    "        ite += 1\n",
    "    return concensus, errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat, vec = gen_matrices(1000)[0:2]\n",
    "res, errs = grad_admm_dirty(mat, vec, 1, 0.005, 1e-5, 10000)\n",
    "errs = np.array(errs)\n",
    "errs[:, -1] = errs[:, -1] - errs[-1, -1]\n",
    "pd.DataFrame(errs).apply(np.log).plot()\n",
    "# plt.plot(np.log(errs))\n",
    "np.abs(res).max(), np.abs(res).sum()  # , res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-square",
   "metadata": {},
   "source": [
    "# Concensus ADMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inappropriate-learning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concensus_admm_dirty(\n",
    "    mat, vec, rad_1, rad_infty, err_max=1e-5, ite_max=1000):\n",
    "    rho = 0.001\n",
    "    dim = len(mat)\n",
    "    sqr = np.matmul(mat.T, mat)\n",
    "    mul_mat = inv(np.eye(dim) + rho * sqr)\n",
    "    out = rho * np.matmul(mul_mat, np.matmul(mat.T, vec))\n",
    "    concensus = np.zeros(dim)\n",
    "    dual = np.zeros(dim)\n",
    "    dual_1 = np.zeros(dim)\n",
    "    dual_2 = np.zeros(dim)\n",
    "#     v_min = -rad_infty\n",
    "#     v_max = +rad_infty\n",
    "    err = err_max + 1\n",
    "    ite = 0\n",
    "    errs = list()\n",
    "    while err > err_max and ite < ite_max:\n",
    "        prim = np.matmul(mul_mat, concensus - dual) + out\n",
    "        prim_1 = proj_linf_annulus(\n",
    "            concensus - dual_1, -rad_infty, +rad_infty)\n",
    "        prim_2 = proj_l1_ball(concensus - dual_2, rad_1)\n",
    "        concensus_new = 1 / 3 * (prim + prim_1 + prim_2)\n",
    "        dual_new = dual + prim - concensus\n",
    "        dual_1_new = dual_1 + prim_1 - concensus\n",
    "        dual_2_new = dual_2 + prim_2 - concensus\n",
    "        err_prim = [\n",
    "            norm(concensus_new - prim),\n",
    "            norm(concensus_new - prim_1),\n",
    "            norm(concensus_new - prim_2),\n",
    "        ]\n",
    "        err_dual = [\n",
    "            rho * norm(concensus - concensus_new),\n",
    "            rho * norm(prim_1 - concensus),\n",
    "            rho * norm(prim_2 - concensus)]\n",
    "        err = max(err_prim + err_dual)\n",
    "        errs.append(err_prim + err_dual + [norm(mat @ concensus - vec)])\n",
    "        concensus = concensus_new.copy()\n",
    "        dual = dual_new.copy()\n",
    "        dual_1 = dual_1_new.copy()\n",
    "        dual_2 = dual_2_new.copy()\n",
    "        ite += 1\n",
    "    return concensus, errs"
   ]
  },
  {
   "cell_type": "raw",
   "id": "sacred-large",
   "metadata": {},
   "source": [
    "def update_rho_admm(rho, dual, err_prim, err_dual, tau, ratio_max):\n",
    "    ratio_errs = err_prim / err_dual\n",
    "    if ratio_errs > ratio_max:\n",
    "        rho *= tau\n",
    "        dual = dual / tau\n",
    "    elif ratio_errs < 1 / ratio_max:\n",
    "        rho /= tau\n",
    "        dual = dual * tau\n",
    "    return rho, dual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-berry",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat, vec = gen_matrices(1000)[0:2]\n",
    "res, errs = concensus_admm_dirty(mat, vec, 1, 0.005, 1e-5, 10000)\n",
    "errs = np.array(errs)\n",
    "errs[:, -1] = errs[:, -1] - errs[-1, -1]\n",
    "pd.DataFrame(errs).apply(np.log).plot()\n",
    "# plt.plot(np.log(errs))\n",
    "np.abs(res).max(), np.abs(res).sum()  # , res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-associate",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "mat, vec = gen_matrices(1000)[0:2]\n",
    "res, errs = concensus_admm_dirty(mat, vec, 1, 0.005)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
